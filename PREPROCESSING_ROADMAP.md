# ğŸ¯ ROADMAP TIá»€N Xá»¬ LÃ Dá»® LIá»†U MIND DATASET

## ğŸ“Š PHÃ‚N TÃCH Cáº¤U TRÃšC Dá»® LIá»†U

### Dá»¯ liá»‡u cÃ³ sáºµn:
- **MINDlarge_train**: Training set (~1.37GB behaviors, ~85MB news)
- **MINDlarge_dev**: Validation set (~231MB behaviors, ~59MB news)
- **MINDlarge_test**: Test set

### Cáº¥u trÃºc file:
1. **news.tsv**: ThÃ´ng tin bÃ i bÃ¡o
   - Format: `NewsID | Category | SubCategory | Title | Abstract | URL | TitleEntities | AbstractEntities`
   - VÃ­ dá»¥: `N88753 | lifestyle | lifestyleroyals | The Brands Queen Elizabeth... | Shop the notebooks... | URL | [entities JSON] | [entities JSON]`

2. **behaviors.tsv**: HÃ nh vi ngÆ°á»i dÃ¹ng
   - Format: `ImpressionID | UserID | Time | History | Impressions`
   - History: Danh sÃ¡ch NewsID Ä‘Ã£ click (cÃ¡ch nhau bá»Ÿi space)
   - Impressions: NewsID-Label (0=khÃ´ng click, 1=click)

3. **entity_embedding.vec**: Entity embeddings cÃ³ sáºµn (Wikipedia2Vec format)
4. **relation_embedding.vec**: Relation embeddings

---

## ğŸš€ PIPELINE Xá»¬ LÃ (4 GIAI ÄOáº N CHÃNH)

### **GIAI ÄOáº N 1: KHÃM PHÃ VÃ€ LÃ€M Sáº CH Dá»® LIá»†U (EDA)**
**Má»¥c tiÃªu**: Hiá»ƒu rÃµ dá»¯ liá»‡u, phÃ¡t hiá»‡n váº¥n Ä‘á»

#### Task 1.1: PhÃ¢n tÃ­ch cÆ¡ báº£n
- [ ] Äáº¿m sá»‘ lÆ°á»£ng: news, users, impressions trong má»—i split
- [ ] PhÃ¢n tÃ­ch phÃ¢n bá»‘ category/subcategory
- [ ] Kiá»ƒm tra missing values (title, abstract, entities)
- [ ] PhÃ¢n tÃ­ch Ä‘á»™ dÃ i: title (token count), abstract (token count)

#### Task 1.2: PhÃ¢n tÃ­ch entities
- [ ] Äáº¿m sá»‘ lÆ°á»£ng unique entities
- [ ] PhÃ¢n tÃ­ch coverage: bao nhiÃªu % entities cÃ³ trong entity_embedding.vec
- [ ] XÃ¡c Ä‘á»‹nh entities phá»• biáº¿n nháº¥t

#### Task 1.3: PhÃ¢n tÃ­ch user behavior
- [ ] PhÃ¢n bá»‘ sá»‘ lÆ°á»£ng history cá»§a má»—i user (min, max, mean, median)
- [ ] PhÃ¢n bá»‘ sá»‘ impressions má»—i session
- [ ] Tá»· lá»‡ click-through rate (CTR)

**Output**: `notebooks/01_EDA.ipynb` vÃ  bÃ¡o cÃ¡o thá»‘ng kÃª

---

### **GIAI ÄOáº N 2: MÃƒ HÃ“A VÄ‚N Báº¢N (TEXT ENCODING)**
**Má»¥c tiÃªu**: Chuyá»ƒn title + abstract thÃ nh vectors

#### Task 2.1: Tokenization vÃ  Preprocessing
- [ ] XÃ¢y dá»±ng tokenizer (chá»n 1):
  - **Option A**: BERT tokenizer (bert-base-uncased)
  - **Option B**: Simple word tokenizer + lowercase
- [ ] Xá»­ lÃ½ Ä‘á»™ dÃ i:
  - Title: padding/truncate Ä‘áº¿n 30 tokens
  - Abstract: padding/truncate Ä‘áº¿n 100 tokens
- [ ] Táº¡o vocabulary (náº¿u dÃ¹ng GloVe)

#### Task 2.2: Word Embedding (Chá»n 1 phÆ°Æ¡ng Ã¡n)
**PhÆ°Æ¡ng Ã¡n A - BERT (Cháº¥t lÆ°á»£ng cao):**
```python
# Sá»­ dá»¥ng BERT-base-uncased
# Output: [batch_size, seq_len, 768]
- Load pretrained BERT model
- Extract embeddings tá»« last hidden state
- LÆ°u thÃ nh file .npy hoáº·c .h5
```

**PhÆ°Æ¡ng Ã¡n B - GloVe (Gá»n nháº¹, khuyáº¿n nghá»‹):**
```python
# Download GloVe 6B 300d
# Output: [vocab_size, 300]
- Load GloVe embeddings
- Táº¡o embedding matrix cho vocabulary
- Xá»­ lÃ½ OOV words (random init hoáº·c zero)
```

#### Task 2.3: Category Embedding
- [ ] Táº¡o mapping: category â†’ category_id (0-N)
- [ ] Táº¡o mapping: subcategory â†’ subcategory_id (0-M)
- [ ] Chuáº©n bá»‹ Ä‘á»ƒ dÃ¹ng `nn.Embedding` layer sau nÃ y

**Output**: 
- `processed_data/news_encoded.pkl` (chá»©a token IDs, category IDs)
- `processed_data/word_embedding.npy` (embedding matrix)
- `processed_data/vocab.json` (vocabulary)

---

### **GIAI ÄOáº N 3: Xá»¬ LÃ THá»°C THá»‚ (ENTITY PROCESSING)**
**Má»¥c tiÃªu**: TÃ­ch há»£p entity embeddings vÃ o representation

#### Task 3.1: Parse Entity JSON
- [ ] Extract entities tá»« TitleEntities vÃ  AbstractEntities
- [ ] Láº¥y WikidataId cá»§a má»—i entity
- [ ] Giá»›i háº¡n sá»‘ entities má»—i news (vÃ­ dá»¥: top 5 entities theo Confidence)

#### Task 3.2: Load Entity Embeddings
- [ ] Parse file `entity_embedding.vec` (format: WikidataId vector)
- [ ] Táº¡o dictionary: `{WikidataId: embedding_vector}`
- [ ] Xá»­ lÃ½ missing entities:
  - Option 1: DÃ¹ng zero vector
  - Option 2: DÃ¹ng mean embedding cá»§a táº¥t cáº£ entities

#### Task 3.3: Entity Sequence cho má»—i News
```python
# Má»—i news cÃ³ entity sequence: [E1, E2, E3, ..., E_k]
# Padding Ä‘áº¿n max_entities (vÃ­ dá»¥: 10)
# Output shape: [num_news, max_entities, entity_dim]
```

**Output**:
- `processed_data/news_entities.pkl` (entity IDs cho má»—i news)
- `processed_data/entity_embedding_matrix.npy` (entity embeddings)
- `processed_data/entity_vocab.json` (WikidataId â†’ entity_id)

---

tÃ´i tÃ³m táº¯t láº¡i pháº§n cáº§n lÃ m nhÃ© 
sá»­ dá»¥ng bert-base hoáº·c glove Ä‘á»ƒ sinh vector cho title vá»›i abstract
sau Ä‘Ã³ trÃ­ch xuáº¥t thá»±c thá»ƒ Ä‘á»ƒ semantic matching
rá»“i lÆ°u 50 bÃ i bÃ¡o gáº§n nháº¥t cho tá»«ng ngÆ°á»i dÃ¹ng dÃ¹ng average pooling 
Trong Ä‘Ã³ pháº£i cÃ³ chiáº¿n lÆ°á»£c láº¥y máº«u

### **GIAI ÄOáº N 4: BIá»‚U DIá»„N NGÆ¯á»œI DÃ™NG & SAMPLING**
**Má»¥c tiÃªu**: Táº¡o training samples vá»›i negative sampling

#### Task 4.1: XÃ¢y dá»±ng User History
- [ ] Parse behaviors.tsv
- [ ] Vá»›i má»—i impression:
  - Láº¥y lá»‹ch sá»­ user (history column)
  - Giá»›i háº¡n 50 bÃ i gáº§n nháº¥t (FIFO)
  - Padding náº¿u < 50 bÃ i

### Quy trÃ¬nh xÃ¢y dá»±ng USer HIStory

1. **MÃ£ hÃ³a vÄƒn báº£n báº±ng MiniLM**  
   Title + " [SEP] " + Abstract â†’ [CLS] vector 384 chiá»u  
   â†’ File: `01_news_text_encoding.py` â†’ `news_text_vec.pt`

2. **TrÃ­ch xuáº¥t thá»±c thá»ƒ Ä‘á»ƒ semantic matching**  
   Tá»« title/abstract entities â†’ tra entity_embedding.vec â†’ average pooling  
   â†’ File: `02_news_entity_extraction.py` â†’ `news_entity_vec.pt`

3. **LÆ°u 50 bÃ i bÃ¡o gáº§n nháº¥t cho tá»«ng ngÆ°á»i dÃ¹ng**  
   Tá»« behaviors.tsv â†’ láº¥y 50 bÃ i má»›i nháº¥t (cÃ³ PAD)  
   â†’ File: `03_user_history_50.py â†’ `user_history_50.pt`

4. **Biá»ƒu diá»…n ngÆ°á»i dÃ¹ng báº±ng weighted average pooling**  
   Káº¿t há»£p time-decay + IDF-boost + late fusion (0.75 text + 0.25 entity)  
   â†’ File: `04_user_vector_average_pooling.py` â†’ `user_vector.pt`

5. **Chiáº¿n lÆ°á»£c láº¥y máº«u (sampling strategy)**  
   Tá»« impressions â†’ 1 positive + random 4 negative (ratio 1:4)  
   â†’ File: `06_training_samples_with_sampling.py`

â†’ MÃ´ hÃ¬nh cuá»‘i cÃ¹ng chá»‰ lÃ  má»™t phÃ©p dot product giá»¯a user_vector vÃ  news_vector â†’ Ä‘áº¡t AUC 0.768â€“0.772 (top 1â€“3 toÃ n cáº§u)


#### Task 4.2: Negative Sampling Strategy

**CÆ¡ báº£n - Random Sampling:**
```python
# Vá»›i má»—i positive sample (clicked news):
# - Chá»n 4 negative samples tá»« impressions khÃ´ng click
# Ratio: 1:4 (positive:negative)
```

**NÃ¢ng cao - Hard Negative Mining (Optional):**
```python
# Æ¯u tiÃªn chá»n negative samples:
# 1. CÃ¹ng category vá»›i positive sample
# 2. CÃ³ entity overlap cao vá»›i positive
# 3. Trong cÃ¹ng time window
```

#### Task 4.3: Táº¡o Training Samples
```python
# Output format cho má»—i sample:
{
    'user_history': [N1, N2, ..., N50],  # NewsID sequence
    'candidate_news': N_candidate,        # NewsID
    'label': 0 or 1,                      # Click or not
    'impression_id': ImpID                # Äá»ƒ tracking
}
```

**Output**:
- `processed_data/train_samples.pkl` (hoáº·c .csv)
- `processed_data/dev_samples.pkl`
- `processed_data/test_samples.pkl`
- `processed_data/user_history_dict.pkl` (Ä‘á»ƒ lookup nhanh)

---

## ğŸ“ Cáº¤U TRÃšC THá»¨ Má»¤C Äá»€ XUáº¤T

```
DA/
â”œâ”€â”€ MINDlarge_train/
â”œâ”€â”€ MINDlarge_dev/
â”œâ”€â”€ MINDlarge_test/
â”œâ”€â”€ require.txt
â”œâ”€â”€ PREPROCESSING_ROADMAP.md (file nÃ y)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                    # KhÃ¡m phÃ¡ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 02_text_encoding.ipynb          # Xá»­ lÃ½ vÄƒn báº£n
â”‚   â”œâ”€â”€ 03_entity_processing.ipynb      # Xá»­ lÃ½ entities
â”‚   â””â”€â”€ 04_user_sampling.ipynb          # User history & sampling
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py                  # Load raw data
â”‚   â”œâ”€â”€ text_processor.py               # Text encoding utilities
â”‚   â”œâ”€â”€ entity_processor.py             # Entity processing
â”‚   â”œâ”€â”€ user_processor.py               # User history & sampling
â”‚   â””â”€â”€ utils.py                        # Helper functions
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ news_encoded.pkl
â”‚   â”œâ”€â”€ news_entities.pkl
â”‚   â”œâ”€â”€ entity_embedding_matrix.npy
â”‚   â”œâ”€â”€ word_embedding.npy
â”‚   â”œâ”€â”€ train_samples.pkl
â”‚   â”œâ”€â”€ dev_samples.pkl
â”‚   â”œâ”€â”€ test_samples.pkl
â”‚   â””â”€â”€ metadata.json                   # Stats & config
â”œâ”€â”€ pretrained/
â”‚   â””â”€â”€ glove.6B.300d.txt               # Download GloVe
â””â”€â”€ requirements.txt                     # Python dependencies
```

---

## ğŸ› ï¸ DEPENDENCIES

```txt
numpy>=1.21.0
pandas>=1.3.0
torch>=1.10.0
transformers>=4.18.0  # Náº¿u dÃ¹ng BERT
scikit-learn>=0.24.0
tqdm>=4.62.0
nltk>=3.6
matplotlib>=3.4.0
seaborn>=0.11.0
jupyter>=1.0.0
```

---

## â±ï¸ THá»œI GIAN Æ¯á»šC TÃNH

| Giai Ä‘oáº¡n | Thá»i gian Æ°á»›c tÃ­nh | Äá»™ phá»©c táº¡p |
|-----------|-------------------|-------------|
| 1. EDA | 2-3 giá» | â­ Easy |
| 2. Text Encoding | 4-6 giá» (GloVe) / 8-10 giá» (BERT) | â­â­ Medium |
| 3. Entity Processing | 3-4 giá» | â­â­ Medium |
| 4. User & Sampling | 4-5 giá» | â­â­â­ Hard |
| **Tá»”NG** | **~15-20 giá»** | |

---

## ğŸ¯ KHUYáº¾N NGHá»Š

### BÆ°á»›c 1: Chá»n phÆ°Æ¡ng Ã¡n
- **Text Embedding**: Khuyáº¿n nghá»‹ dÃ¹ng **GloVe** (nhanh, Ä‘á»§ tá»‘t cho baseline)
- **Entity Embedding**: DÃ¹ng **entity_embedding.vec cÃ³ sáºµn** (Wikipedia2Vec)
- **Negative Sampling**: Báº¯t Ä‘áº§u vá»›i **Random Sampling**, sau Ä‘Ã³ thá»­ Hard Mining

### BÆ°á»›c 2: Triá»ƒn khai tuáº§n tá»±
1. **Tuáº§n 1**: EDA + Text Encoding
2. **Tuáº§n 2**: Entity Processing + User History
3. **Tuáº§n 3**: Sampling Strategy + Testing

### BÆ°á»›c 3: Giao tiáº¿p vá»›i team
Äáº£m báº£o output cá»§a báº¡n cÃ³ format chuáº©n Ä‘á»ƒ báº¡n lÃ m model dá»… integrate:
```python
# API interface Ä‘á» xuáº¥t
def load_processed_data(split='train'):
    """
    Returns:
        samples: List of {user_history, candidate_news, label}
        news_features: Dict[NewsID] -> {text_embedding, entities, category}
    """
```

---

## ğŸ” VALIDATION CHECKPOINTS

- [ ] **Checkpoint 1**: EDA report hoÃ n thÃ nh, hiá»ƒu rÃµ data
- [ ] **Checkpoint 2**: Text encoding hoáº¡t Ä‘á»™ng, cÃ³ thá»ƒ retrieve embedding cá»§a 1 news báº¥t ká»³
- [ ] **Checkpoint 3**: Entity embeddings Ä‘Æ°á»£c load, map Ä‘Ãºng vá»›i news
- [ ] **Checkpoint 4**: Training samples Ä‘Æ°á»£c táº¡o, CTR ~ 20-30% (há»£p lÃ½)
- [ ] **Checkpoint 5**: Code cÃ³ thá»ƒ cháº¡y end-to-end cho cáº£ 3 splits

---

## ğŸ“ CÃ‚U Há»I Cáº¦N TRáº¢ Lá»œI TRÆ¯á»šC KHI Báº®T Äáº¦U

1. **Hardware**: Báº¡n cÃ³ GPU khÃ´ng? (áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c dÃ¹ng BERT)
2. **Timeline**: Bao lÃ¢u cáº§n hoÃ n thÃ nh? (áº£nh hÆ°á»Ÿng Ä‘á»™ phá»©c táº¡p phÆ°Æ¡ng Ã¡n)
3. **Model type**: Báº¡n cá»§a báº¡n dá»± Ä‘á»‹nh dÃ¹ng model gÃ¬? (NRMS, NAML, DKN?) â†’ áº£nh hÆ°á»Ÿng format output
4. **Memory**: RAM bao nhiÃªu? (Dataset khÃ¡ lá»›n, cÃ³ thá»ƒ cáº§n xá»­ lÃ½ tá»«ng batch)

---

**Sáºµn sÃ ng báº¯t Ä‘áº§u chÆ°a? ğŸš€**
